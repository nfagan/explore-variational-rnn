{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGIC_OPERATIONS = {\n",
    "    'AND': lambda x, y: x & y,  \n",
    "    # Logical AND: True if both x and y are True; otherwise, False.\n",
    "    \n",
    "    'OR': lambda x, y: x | y,  \n",
    "    # Logical OR: True if at least one of x or y is True.\n",
    "    \n",
    "    'XOR': lambda x, y: x ^ y,  \n",
    "    # Exclusive OR: True if x and y are different, False if they are the same.\n",
    "    \n",
    "    'NAND': lambda x, y: ~(x & y) & 1,  \n",
    "    # NOT AND: Inverse of AND; True unless both x and y are True.\n",
    "    \n",
    "    'NOR': lambda x, y: ~(x | y) & 1,  \n",
    "    # NOT OR: Inverse of OR; True only if both x and y are False.\n",
    "    \n",
    "    'XNOR': lambda x, y: ~(x ^ y) & 1,  \n",
    "    # Logical Equivalence (XNOR): True if x and y are the same.\n",
    "    \n",
    "    'IMPLIES': lambda x, y: (~x | y) & 1,  \n",
    "    # Logical Implication (if/then, P → Q): False only when x is True and y is False.\n",
    "    \n",
    "    'REVERSE_IMPLIES': lambda x, y: (x | ~y) & 1,  \n",
    "    # Reverse Implication (then/if, Q → P): False only when y is True and x is False.\n",
    "    \n",
    "    'XQ': lambda x, y: (~x & y) & 1,  \n",
    "    # Custom logic from the paper: True only if x is False and y is True.\n",
    "    \n",
    "    'ABJ': lambda x, y: (x & ~y) & 1  \n",
    "    # Material Nonimplication (Abjunction, P ⊅ Q): True if x is True and y is False.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AND\n",
       "0 0 0\n",
       "0 1 0\n",
       "1 0 0\n",
       "1 1 1\n",
       "\n",
       "OR\n",
       "0 0 0\n",
       "0 1 1\n",
       "1 0 1\n",
       "1 1 1\n",
       "\n",
       "XOR\n",
       "0 0 0\n",
       "0 1 1\n",
       "1 0 1\n",
       "1 1 0\n",
       "\n",
       "NAND\n",
       "0 0 1\n",
       "0 1 1\n",
       "1 0 1\n",
       "1 1 0\n",
       "\n",
       "NOR\n",
       "0 0 1\n",
       "0 1 0\n",
       "1 0 0\n",
       "1 1 0\n",
       "\n",
       "XNOR\n",
       "0 0 1\n",
       "0 1 0\n",
       "1 0 0\n",
       "1 1 1\n",
       "\n",
       "IMPLIES\n",
       "0 0 1\n",
       "0 1 1\n",
       "1 0 0\n",
       "1 1 1\n",
       "\n",
       "REVERSE_IMPLIES\n",
       "0 0 1\n",
       "0 1 0\n",
       "1 0 1\n",
       "1 1 1\n",
       "\n",
       "XQ\n",
       "0 0 0\n",
       "0 1 1\n",
       "1 0 0\n",
       "1 1 0\n",
       "\n",
       "ABJ\n",
       "0 0 0\n",
       "0 1 0\n",
       "1 0 1\n",
       "1 1 0\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for key in LOGIC_OPERATIONS.keys():\n",
    "    print(key)\n",
    "    for x in [0, 1]:\n",
    "        for y in [0, 1]:\n",
    "            print(x, y, LOGIC_OPERATIONS[key](x, y))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogicDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    A logic operation dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_samples = 10000,\n",
    "            num_operations = 10,\n",
    "            seq_len = 10,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "        \"\"\"\n",
    "\n",
    "        self.num_samples = num_samples\n",
    "        self.num_operations = num_operations\n",
    "        self.seq_len = seq_len\n",
    "        self.operation_list = list(LOGIC_OPERATIONS.keys())\n",
    "        \n",
    "        # generate data\n",
    "        self.generate_data()\n",
    "\n",
    "\n",
    "    def generate_data(self):\n",
    "        \"\"\"\n",
    "        Generate dataset.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.symbols = []\n",
    "        self.operations = []\n",
    "        \n",
    "        for _ in range(self.num_samples):\n",
    "            # sample initial variables\n",
    "            s1, s2 = np.random.choice([0, 1], size = 2)\n",
    "\n",
    "            # sample operations\n",
    "            operation_indices = np.random.randint(0, len(self.operation_list), size = self.num_operations)\n",
    "\n",
    "            # compute ground truth output\n",
    "            result = s1\n",
    "            for idx in operation_indices:\n",
    "                result = LOGIC_OPERATIONS[self.operation_list[idx]](result, s2)\n",
    "                result = int(result)\n",
    "\n",
    "                if result not in [0, 1]:\n",
    "                    raise ValueError('Result is not boolean.')\n",
    "\n",
    "            # convert operations to one-hot encoding using torch\n",
    "            operations = F.one_hot(torch.tensor(operation_indices), num_classes = len(self.operation_list)).float() # (seq_len, num_operations)\n",
    "            operations = operations.reshape(-1) # (seq_len * num_operation,)\n",
    "            \n",
    "            # create tensors\n",
    "            input = torch.cat([torch.tensor([s1, s2], dtype = torch.float32), operations], dim = 0) # (seq_len * num_operations + 2,)\n",
    "            label = torch.tensor(result, dtype = torch.long) # integer class index\n",
    "\n",
    "            # append data and label\n",
    "            self.data.append(input)\n",
    "            self.labels.append(label)\n",
    "            self.symbols.append([s1, s2])\n",
    "            self.operations.append(list(operation_indices))\n",
    "        \n",
    "        self.data = torch.stack(self.data)\n",
    "        self.data = self.data.unsqueeze(1).repeat(1, self.seq_len, 1) # (num_samples, seq_len, feature_size)\n",
    "        self.labels = torch.tensor(self.labels) # (num_samples)\n",
    "\n",
    "        self.symbols = torch.tensor(self.symbols)\n",
    "        self.operations = torch.tensor(self.operations)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30000, 10, 102])\n",
       "torch.Size([30000])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = LogicDataset(\n",
    "    num_samples = 30000,\n",
    "    num_operations = 10,\n",
    "    seq_len = 10,\n",
    ")\n",
    "print(dataset.data.shape)\n",
    "print(dataset.labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogicGRU(nn.Module):\n",
    "    \"\"\"\n",
    "    A GRU network class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size,\n",
    "            hidden_size = 128,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Initialize the network.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(LogicGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first = True)\n",
    "        self.fc = nn.Linear(hidden_size, 2)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward the network.\n",
    "        \n",
    "        Args:\n",
    "            x: a torch.tensor with a shape of (batch_size, seq_len, input_size).\n",
    "        \n",
    "        Returns:\n",
    "            outputs: a torch.tensor with a shape of (batch_size, seq_len, 2)\n",
    "            hiddens: a torch.tensor with a shape of (batch_size, seq_len, hidden_size)\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "        hidden_init = torch.zeros(1, batch_size, self.hidden_size) # (layer_size, batch_size, hidden_size)\n",
    "        hiddens, _ = self.gru(x, hidden_init) # (batch_size, seq_len, hidden_size)\n",
    "        outputs = self.fc(hiddens) # (batch_size, seq_len, 2)\n",
    "\n",
    "        return outputs, hiddens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \"\"\"\n",
    "    A trainer class.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            model,\n",
    "            train_loader,\n",
    "            lr = 1e-3,\n",
    "            device = 'cpu'\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Initialize the trainer.\n",
    "        \"\"\"\n",
    "\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.device = device\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr = lr)\n",
    "\n",
    "\n",
    "    def train(self, num_epochs):\n",
    "        \"\"\"\n",
    "        Train the network.\n",
    "        \"\"\"\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_loss = 0\n",
    "            for inputs, targets in self.train_loader:\n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs, _ = self.model(inputs) # (batch_size, seq_len, 2)\n",
    "                loss = self.criterion(outputs[:, -1, :], targets)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            print(f'Epoch {epoch}, Loss: {epoch_loss / len(self.train_loader):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Epoch 0, Loss: 0.6847\n",
       "Epoch 1, Loss: 0.6446\n",
       "Epoch 2, Loss: 0.5989\n",
       "Epoch 3, Loss: 0.5892\n",
       "Epoch 4, Loss: 0.5871\n",
       "Epoch 5, Loss: 0.5843\n",
       "Epoch 6, Loss: 0.5834\n",
       "Epoch 7, Loss: 0.5807\n",
       "Epoch 8, Loss: 0.5767\n",
       "Epoch 9, Loss: 0.5691\n",
       "Epoch 10, Loss: 0.5442\n",
       "Epoch 11, Loss: 0.4754\n",
       "Epoch 12, Loss: 0.4434\n",
       "Epoch 13, Loss: 0.4214\n",
       "Epoch 14, Loss: 0.4037\n",
       "Epoch 15, Loss: 0.3904\n",
       "Epoch 16, Loss: 0.3747\n",
       "Epoch 17, Loss: 0.3548\n",
       "Epoch 18, Loss: 0.3421\n",
       "Epoch 19, Loss: 0.3205\n",
       "Epoch 20, Loss: 0.3131\n",
       "Epoch 21, Loss: 0.2983\n",
       "Epoch 22, Loss: 0.2752\n",
       "Epoch 23, Loss: 0.2557\n",
       "Epoch 24, Loss: 0.2425\n",
       "Epoch 25, Loss: 0.2293\n",
       "Epoch 26, Loss: 0.2289\n",
       "Epoch 27, Loss: 0.2246\n",
       "Epoch 28, Loss: 0.2079\n",
       "Epoch 29, Loss: 0.2041\n",
       "Epoch 30, Loss: 0.1970\n",
       "Epoch 31, Loss: 0.1994\n",
       "Epoch 32, Loss: 0.1785\n",
       "Epoch 33, Loss: 0.1669\n",
       "Epoch 34, Loss: 0.1705\n",
       "Epoch 35, Loss: 0.1914\n",
       "Epoch 36, Loss: 0.1648\n",
       "Epoch 37, Loss: 0.1462\n",
       "Epoch 38, Loss: 0.1324\n",
       "Epoch 39, Loss: 0.1277\n",
       "Epoch 40, Loss: 0.1441\n",
       "Epoch 41, Loss: 0.1388\n",
       "Epoch 42, Loss: 0.1226\n",
       "Epoch 43, Loss: 0.1226\n",
       "Epoch 44, Loss: 0.1052\n",
       "Epoch 45, Loss: 0.1024\n",
       "Epoch 46, Loss: 0.0881\n",
       "Epoch 47, Loss: 0.0908\n",
       "Epoch 48, Loss: 0.0934\n",
       "Epoch 49, Loss: 0.0951\n",
       "Epoch 50, Loss: 0.1012\n",
       "Epoch 51, Loss: 0.0834\n",
       "Epoch 52, Loss: 0.0641\n",
       "Epoch 53, Loss: 0.0535\n",
       "Epoch 54, Loss: 0.0466\n",
       "Epoch 55, Loss: 0.0453\n",
       "Epoch 56, Loss: 0.0497\n",
       "Epoch 57, Loss: 0.0644\n",
       "Epoch 58, Loss: 0.1032\n",
       "Epoch 59, Loss: 0.0819\n",
       "Epoch 60, Loss: 0.0625\n",
       "Epoch 61, Loss: 0.0461\n",
       "Epoch 62, Loss: 0.0345\n",
       "Epoch 63, Loss: 0.0251\n",
       "Epoch 64, Loss: 0.0200\n",
       "Epoch 65, Loss: 0.0184\n",
       "Epoch 66, Loss: 0.0143\n",
       "Epoch 67, Loss: 0.0149\n",
       "Epoch 68, Loss: 0.0179\n",
       "Epoch 69, Loss: 0.0248\n",
       "Epoch 70, Loss: 0.0415\n",
       "Epoch 71, Loss: 0.0718\n",
       "Epoch 72, Loss: 0.0755\n",
       "Epoch 73, Loss: 0.0646\n",
       "Epoch 74, Loss: 0.0447\n",
       "Epoch 75, Loss: 0.0259\n",
       "Epoch 76, Loss: 0.0154\n",
       "Epoch 77, Loss: 0.0122\n",
       "Epoch 78, Loss: 0.0079\n",
       "Epoch 79, Loss: 0.0049\n",
       "Epoch 80, Loss: 0.0035\n",
       "Epoch 81, Loss: 0.0033\n",
       "Epoch 82, Loss: 0.0025\n",
       "Epoch 83, Loss: 0.0025\n",
       "Epoch 84, Loss: 0.0019\n",
       "Epoch 85, Loss: 0.0019\n",
       "Epoch 86, Loss: 0.0014\n",
       "Epoch 87, Loss: 0.0011\n",
       "Epoch 88, Loss: 0.0008\n",
       "Epoch 89, Loss: 0.0007\n",
       "Epoch 90, Loss: 0.0007\n",
       "Epoch 91, Loss: 0.0006\n",
       "Epoch 92, Loss: 0.0005\n",
       "Epoch 93, Loss: 0.0005\n",
       "Epoch 94, Loss: 0.0004\n",
       "Epoch 95, Loss: 0.0004\n",
       "Epoch 96, Loss: 0.0004\n",
       "Epoch 97, Loss: 0.0004\n",
       "Epoch 98, Loss: 0.0003\n",
       "Epoch 99, Loss: 0.0003\n",
       "Epoch 100, Loss: 0.0003\n",
       "Epoch 101, Loss: 0.0003\n",
       "Epoch 102, Loss: 0.0003\n",
       "Epoch 103, Loss: 0.0003\n",
       "Epoch 104, Loss: 0.0003\n",
       "Epoch 105, Loss: 0.0002\n",
       "Epoch 106, Loss: 0.0002\n",
       "Epoch 107, Loss: 0.0002\n",
       "Epoch 108, Loss: 0.0002\n",
       "Epoch 109, Loss: 0.0002\n",
       "Epoch 110, Loss: 0.0002\n",
       "Epoch 111, Loss: 0.0002\n",
       "Epoch 112, Loss: 0.0002\n",
       "Epoch 113, Loss: 0.0002\n",
       "Epoch 114, Loss: 0.0002\n",
       "Epoch 115, Loss: 0.0002\n",
       "Epoch 116, Loss: 0.0002\n",
       "Epoch 117, Loss: 0.0002\n",
       "Epoch 118, Loss: 0.0001\n",
       "Epoch 119, Loss: 0.0001\n",
       "Epoch 120, Loss: 0.0001\n",
       "Epoch 121, Loss: 0.0001\n",
       "Epoch 122, Loss: 0.0001\n",
       "Epoch 123, Loss: 0.0001\n",
       "Epoch 124, Loss: 0.0001\n",
       "Epoch 125, Loss: 0.0001\n",
       "Epoch 126, Loss: 0.0001\n",
       "Epoch 127, Loss: 0.0001\n",
       "Epoch 128, Loss: 0.0001\n",
       "Epoch 129, Loss: 0.0001\n",
       "Epoch 130, Loss: 0.0001\n",
       "Epoch 131, Loss: 0.0001\n",
       "Epoch 132, Loss: 0.0001\n",
       "Epoch 133, Loss: 0.0001\n",
       "Epoch 134, Loss: 0.0001\n",
       "Epoch 135, Loss: 0.0001\n",
       "Epoch 136, Loss: 0.0001\n",
       "Epoch 137, Loss: 0.0001\n",
       "Epoch 138, Loss: 0.0001\n",
       "Epoch 139, Loss: 0.0001\n",
       "Epoch 140, Loss: 0.0001\n",
       "Epoch 141, Loss: 0.0001\n",
       "Epoch 142, Loss: 0.0001\n",
       "Epoch 143, Loss: 0.0001\n",
       "Epoch 144, Loss: 0.0001\n",
       "Epoch 145, Loss: 0.0001\n",
       "Epoch 146, Loss: 0.0001\n",
       "Epoch 147, Loss: 0.0001\n",
       "Epoch 148, Loss: 0.0001\n",
       "Epoch 149, Loss: 0.0001\n",
       "Epoch 150, Loss: 0.0001\n",
       "Epoch 151, Loss: 0.0001\n",
       "Epoch 152, Loss: 0.0001\n",
       "Epoch 153, Loss: 0.0001\n",
       "Epoch 154, Loss: 0.0001\n",
       "Epoch 155, Loss: 0.0001\n",
       "Epoch 156, Loss: 0.0001\n",
       "Epoch 157, Loss: 0.0001\n",
       "Epoch 158, Loss: 0.0001\n",
       "Epoch 159, Loss: 0.0001\n",
       "Epoch 160, Loss: 0.0001\n",
       "Epoch 161, Loss: 0.0001\n",
       "Epoch 162, Loss: 0.0001\n",
       "Epoch 163, Loss: 0.0001\n",
       "Epoch 164, Loss: 0.0001\n",
       "Epoch 165, Loss: 0.0001\n",
       "Epoch 166, Loss: 0.0000\n",
       "Epoch 167, Loss: 0.0000\n",
       "Epoch 168, Loss: 0.0000\n",
       "Epoch 169, Loss: 0.0000\n",
       "Epoch 170, Loss: 0.0000\n",
       "Epoch 171, Loss: 0.0000\n",
       "Epoch 172, Loss: 0.0000\n",
       "Epoch 173, Loss: 0.0000\n",
       "Epoch 174, Loss: 0.0000\n",
       "Epoch 175, Loss: 0.0000\n",
       "Epoch 176, Loss: 0.0000\n",
       "Epoch 177, Loss: 0.0000\n",
       "Epoch 178, Loss: 0.0000\n",
       "Epoch 179, Loss: 0.0000\n",
       "Epoch 180, Loss: 0.0000\n",
       "Epoch 181, Loss: 0.0000\n",
       "Epoch 182, Loss: 0.0000\n",
       "Epoch 183, Loss: 0.0000\n",
       "Epoch 184, Loss: 0.0000\n",
       "Epoch 185, Loss: 0.0000\n",
       "Epoch 186, Loss: 0.0000\n",
       "Epoch 187, Loss: 0.0000\n",
       "Epoch 188, Loss: 0.0000\n",
       "Epoch 189, Loss: 0.0000\n",
       "Epoch 190, Loss: 0.0000\n",
       "Epoch 191, Loss: 0.0000\n",
       "Epoch 192, Loss: 0.0000\n",
       "Epoch 193, Loss: 0.0000\n",
       "Epoch 194, Loss: 0.0000\n",
       "Epoch 195, Loss: 0.0000\n",
       "Epoch 196, Loss: 0.0000\n",
       "Epoch 197, Loss: 0.0000\n",
       "Epoch 198, Loss: 0.0000\n",
       "Epoch 199, Loss: 0.0000\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize dataset and dataloader\n",
    "dataset = LogicDataset(\n",
    "    num_samples = 10000,\n",
    "    num_operations = 10,\n",
    "    seq_len = 10,\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size = 1024, shuffle = True)\n",
    "\n",
    "# define model and trainer\n",
    "model = LogicGRU(\n",
    "    input_size = 102,\n",
    "    hidden_size = 128,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    train_loader = train_loader,\n",
    "    lr = 1e-3,\n",
    ")\n",
    "\n",
    "# train the model\n",
    "trainer.train(num_epochs = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 10, 2])\n",
       "torch.Size([10000, 10, 128])\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_dataset = LogicDataset(\n",
    "    num_samples = 10000,\n",
    "    num_operations = 10,\n",
    "    seq_len = 10,\n",
    ")\n",
    "eval_loader = torch.utils.data.DataLoader(eval_dataset, batch_size = 1024, shuffle = True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = eval_dataset.dataset.data\n",
    "    targets = eval_dataset.dataset.labels\n",
    "\n",
    "    outputs, hiddens = model(inputs) # (num_samples, seq_len, 2) / (num_samples, seq_len, hidden_size)\n",
    "\n",
    "    probs = torch.softmax(outputs, dim = -1)\n",
    "    idxs = torch.argmax(probs, dim = -1)\n",
    "    print(idxs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 10))\n",
    "for i in range(100):\n",
    "    plt.subplot(10, 10, i + 1)\n",
    "    plt.plot(probs[i, :, 1], marker = 'o', markersize = 3)\n",
    "    plt.axhline(y = 0.5, color = 'k', linestyle = '--', linewidth = 2)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.ylim((-0.1, 1.1))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operation_list = list(LOGIC_OPERATIONS.keys())\n",
    "\n",
    "for i in range(10):\n",
    "    s1, s2 = dataset.symbols[i]\n",
    "    operation_indices = dataset.operations[i]\n",
    "\n",
    "    # compute ground truth output\n",
    "    results = []\n",
    "    result = s1\n",
    "    for idx in operation_indices:\n",
    "        result = LOGIC_OPERATIONS[operation_list[idx]](result, s2)\n",
    "        result = int(result)\n",
    "        results.append(result)\n",
    "    \n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 10))\n",
    "for i in range(100):\n",
    "    plt.subplot(10, 10, i + 1)\n",
    "    s1, s2 = dataset.symbols[i]\n",
    "    operation_indices = dataset.operations[i]\n",
    "    results = []\n",
    "    result = s1\n",
    "    for idx in operation_indices:\n",
    "        result = LOGIC_OPERATIONS[operation_list[idx]](result, s2)\n",
    "        result = int(result)\n",
    "        results.append(result)\n",
    "\n",
    "    plt.plot(probs[i, :, 1], marker = 'o', markersize = 3)\n",
    "    plt.plot(results, alpha = 0.6)\n",
    "    plt.axhline(y = 0.5, color = 'k', linestyle = '--', linewidth = 2)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.ylim((-0.1, 1.1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "above = probs[:, :, 1] > 0.5  # shape (10000, 15), bool\n",
    "crossings = above[:, 1:] != above[:, :-1]  # shape (10000, 14), bool\n",
    "num_crossings = crossings.sum(dim=1)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(num_crossings)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
